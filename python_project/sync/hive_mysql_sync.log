2025-08-02 09:55:45,685 - INFO - 发现 19 张待同步表
2025-08-02 09:55:46,020 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 09:55:48,932 - INFO - 表 ads_page_visit_rank 数据同步完成，记录数: 1101
2025-08-02 09:55:49,113 - INFO - 表 ads_pc_entry_stats 结构同步成功
2025-08-02 09:58:12,818 - INFO - 表 ads_pc_entry_stats 数据同步完成，记录数: 239695
2025-08-02 09:58:13,025 - INFO - 表 ads_realtime_path_monitor 结构同步成功
2025-08-02 09:58:13,363 - ERROR - 同步表 ads_realtime_path_monitor 数据失败: (1406, "Data too long for column 'next_page_dist' at row 1")
2025-08-02 09:58:13,363 - ERROR - 表 ads_realtime_path_monitor 同步失败，跳过继续处理其他表
2025-08-02 09:58:13,520 - INFO - 表 ads_shop_path_analysis 结构同步成功
2025-08-02 09:58:19,832 - INFO - 表 ads_shop_path_analysis 数据同步完成，记录数: 9267
2025-08-02 09:58:19,985 - INFO - 表 ads_user_segment 结构同步成功
2025-08-02 09:58:20,213 - INFO - 表 ads_user_segment 数据同步完成，记录数: 2
2025-08-02 09:58:20,386 - INFO - 表 ads_wireless_entry_stats 结构同步成功
2025-08-02 09:58:21,313 - INFO - 表 ads_wireless_entry_stats 数据同步完成，记录数: 1100
2025-08-02 09:58:21,569 - INFO - 表 dwd_page_visit_detail 结构同步成功
2025-08-02 09:58:27,963 - ERROR - 同步表 dwd_page_visit_detail 数据失败: An error occurred while calling o102.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 11) (Long executor driver): java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:233)
	at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:53)
	at org.apache.spark.scheduler.DirectTaskResult$$Lambda$2328/1975222061.apply$mcV$sp(Unknown Source)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1405)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:606)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:233)
	at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:53)
	at org.apache.spark.scheduler.DirectTaskResult$$Lambda$2328/1975222061.apply$mcV$sp(Unknown Source)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1405)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:606)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2025-08-02 09:58:27,964 - ERROR - 表 dwd_page_visit_detail 同步失败，跳过继续处理其他表
2025-08-02 09:58:28,189 - ERROR - 同步表 dwd_user_session 结构失败: (1067, "Invalid default value for 'end_time'")
2025-08-02 09:58:28,189 - ERROR - 表 dwd_user_session 同步失败，跳过继续处理其他表
2025-08-02 09:58:28,716 - INFO - Error while receiving.
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1205, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "F:\Auser\anaconda\envs\python_rely\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-08-02 09:58:28,763 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1205, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "F:\Auser\anaconda\envs\python_rely\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1216, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while receiving
2025-08-02 09:58:28,764 - ERROR - 表 dws_page_path_flow 同步失败，跳过继续处理其他表
2025-08-02 09:58:30,808 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:30,808 - ERROR - 表 dws_page_visit_stats 同步失败，跳过继续处理其他表
2025-08-02 09:58:32,852 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:32,853 - ERROR - 表 dws_traffic_entry_stats 同步失败，跳过继续处理其他表
2025-08-02 09:58:34,898 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:34,898 - ERROR - 表 ods_marketing_activity 同步失败，跳过继续处理其他表
2025-08-02 09:58:36,935 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:36,936 - ERROR - 表 ods_order_info 同步失败，跳过继续处理其他表
2025-08-02 09:58:38,968 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:38,969 - ERROR - 表 ods_page_relationship 同步失败，跳过继续处理其他表
2025-08-02 09:58:41,010 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:41,011 - ERROR - 表 ods_page_visit_log 同步失败，跳过继续处理其他表
2025-08-02 09:58:43,048 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:43,049 - ERROR - 表 ods_product_info 同步失败，跳过继续处理其他表
2025-08-02 09:58:45,086 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:45,087 - ERROR - 表 ods_shop_page_info 同步失败，跳过继续处理其他表
2025-08-02 09:58:47,125 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:47,127 - ERROR - 表 ods_traffic_source 同步失败，跳过继续处理其他表
2025-08-02 09:58:49,167 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:49,168 - ERROR - 表 ods_user_info 同步失败，跳过继续处理其他表
2025-08-02 09:58:51,204 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:53,726 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:55,780 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:57,817 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:58:59,857 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:59:01,897 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:59:03,935 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:59:05,970 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:59:08,008 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:59:10,045 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:59:12,085 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:59:14,121 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 09:59:16,163 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:52587)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:07:01,868 - INFO - 发现 19 张待同步表
2025-08-02 10:07:01,868 - INFO - 开始同步表 ads_page_visit_rank (尝试 1/3)
2025-08-02 10:07:02,175 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:07:03,469 - ERROR - 同步表 ads_page_visit_rank 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:07:03,470 - ERROR - 表 ads_page_visit_rank 同步失败 (尝试 1/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:07:13,482 - INFO - 开始同步表 ads_page_visit_rank (尝试 2/3)
2025-08-02 10:07:13,673 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:07:13,814 - ERROR - 同步表 ads_page_visit_rank 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:07:13,814 - ERROR - 表 ads_page_visit_rank 同步失败 (尝试 2/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:07:33,825 - INFO - 开始同步表 ads_page_visit_rank (尝试 3/3)
2025-08-02 10:07:34,029 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:07:34,218 - ERROR - 同步表 ads_page_visit_rank 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:07:34,218 - ERROR - 表 ads_page_visit_rank 同步失败 (尝试 3/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:07:34,218 - ERROR - 表 ads_page_visit_rank 同步失败，跳过继续处理其他表
2025-08-02 10:07:34,218 - INFO - 开始同步表 ads_pc_entry_stats (尝试 1/3)
2025-08-02 10:07:34,452 - INFO - 表 ads_pc_entry_stats 结构同步成功
2025-08-02 10:07:34,629 - ERROR - 同步表 ads_pc_entry_stats 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:07:34,629 - ERROR - 表 ads_pc_entry_stats 同步失败 (尝试 1/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:07:44,631 - INFO - 开始同步表 ads_pc_entry_stats (尝试 2/3)
2025-08-02 10:07:44,858 - INFO - 表 ads_pc_entry_stats 结构同步成功
2025-08-02 10:07:45,077 - ERROR - 同步表 ads_pc_entry_stats 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:07:45,077 - ERROR - 表 ads_pc_entry_stats 同步失败 (尝试 2/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:05,085 - INFO - 开始同步表 ads_pc_entry_stats (尝试 3/3)
2025-08-02 10:08:05,246 - INFO - 表 ads_pc_entry_stats 结构同步成功
2025-08-02 10:08:05,371 - ERROR - 同步表 ads_pc_entry_stats 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:05,371 - ERROR - 表 ads_pc_entry_stats 同步失败 (尝试 3/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:05,371 - ERROR - 表 ads_pc_entry_stats 同步失败，跳过继续处理其他表
2025-08-02 10:08:05,371 - INFO - 开始同步表 ads_realtime_path_monitor (尝试 1/3)
2025-08-02 10:08:05,569 - INFO - 表 ads_realtime_path_monitor 结构同步成功
2025-08-02 10:08:05,767 - ERROR - 同步表 ads_realtime_path_monitor 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:05,767 - ERROR - 表 ads_realtime_path_monitor 同步失败 (尝试 1/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:15,772 - INFO - 开始同步表 ads_realtime_path_monitor (尝试 2/3)
2025-08-02 10:08:15,942 - INFO - 表 ads_realtime_path_monitor 结构同步成功
2025-08-02 10:08:16,100 - ERROR - 同步表 ads_realtime_path_monitor 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:16,101 - ERROR - 表 ads_realtime_path_monitor 同步失败 (尝试 2/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:36,105 - INFO - 开始同步表 ads_realtime_path_monitor (尝试 3/3)
2025-08-02 10:08:36,268 - INFO - 表 ads_realtime_path_monitor 结构同步成功
2025-08-02 10:08:36,438 - ERROR - 同步表 ads_realtime_path_monitor 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:36,438 - ERROR - 表 ads_realtime_path_monitor 同步失败 (尝试 3/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:36,438 - ERROR - 表 ads_realtime_path_monitor 同步失败，跳过继续处理其他表
2025-08-02 10:08:36,438 - INFO - 开始同步表 ads_shop_path_analysis (尝试 1/3)
2025-08-02 10:08:36,609 - INFO - 表 ads_shop_path_analysis 结构同步成功
2025-08-02 10:08:36,795 - ERROR - 同步表 ads_shop_path_analysis 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:36,795 - ERROR - 表 ads_shop_path_analysis 同步失败 (尝试 1/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:46,807 - INFO - 开始同步表 ads_shop_path_analysis (尝试 2/3)
2025-08-02 10:08:46,955 - INFO - 表 ads_shop_path_analysis 结构同步成功
2025-08-02 10:08:47,082 - ERROR - 同步表 ads_shop_path_analysis 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:47,082 - ERROR - 表 ads_shop_path_analysis 同步失败 (尝试 2/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:08:52,040 - INFO - 同步任务结束
2025-08-02 10:35:31,015 - INFO - 发现 19 张待同步表
2025-08-02 10:35:31,016 - INFO - 开始同步表 ads_page_visit_rank (尝试 1/3)
2025-08-02 10:35:31,327 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:35:32,587 - ERROR - 同步表 ads_page_visit_rank 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:35:32,587 - ERROR - 表 ads_page_visit_rank 同步失败 (尝试 1/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:35:42,590 - INFO - 开始同步表 ads_page_visit_rank (尝试 2/3)
2025-08-02 10:35:42,781 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:35:42,916 - ERROR - 同步表 ads_page_visit_rank 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:35:42,916 - ERROR - 表 ads_page_visit_rank 同步失败 (尝试 2/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:02,929 - INFO - 开始同步表 ads_page_visit_rank (尝试 3/3)
2025-08-02 10:36:03,104 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:36:03,239 - ERROR - 同步表 ads_page_visit_rank 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:03,241 - ERROR - 表 ads_page_visit_rank 同步失败 (尝试 3/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:03,241 - ERROR - 表 ads_page_visit_rank 同步失败，跳过继续处理其他表
2025-08-02 10:36:03,241 - INFO - 开始同步表 ads_pc_entry_stats (尝试 1/3)
2025-08-02 10:36:03,396 - INFO - 表 ads_pc_entry_stats 结构同步成功
2025-08-02 10:36:03,570 - ERROR - 同步表 ads_pc_entry_stats 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:03,570 - ERROR - 表 ads_pc_entry_stats 同步失败 (尝试 1/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:13,582 - INFO - 开始同步表 ads_pc_entry_stats (尝试 2/3)
2025-08-02 10:36:13,731 - INFO - 表 ads_pc_entry_stats 结构同步成功
2025-08-02 10:36:13,864 - ERROR - 同步表 ads_pc_entry_stats 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:13,864 - ERROR - 表 ads_pc_entry_stats 同步失败 (尝试 2/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:33,866 - INFO - 开始同步表 ads_pc_entry_stats (尝试 3/3)
2025-08-02 10:36:33,999 - INFO - 表 ads_pc_entry_stats 结构同步成功
2025-08-02 10:36:34,116 - ERROR - 同步表 ads_pc_entry_stats 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:34,116 - ERROR - 表 ads_pc_entry_stats 同步失败 (尝试 3/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:34,116 - ERROR - 表 ads_pc_entry_stats 同步失败，跳过继续处理其他表
2025-08-02 10:36:34,116 - INFO - 开始同步表 ads_realtime_path_monitor (尝试 1/3)
2025-08-02 10:36:34,320 - INFO - 表 ads_realtime_path_monitor 结构同步成功
2025-08-02 10:36:34,511 - ERROR - 同步表 ads_realtime_path_monitor 数据失败: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:34,511 - ERROR - 表 ads_realtime_path_monitor 同步失败 (尝试 1/3): Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2025-08-02 10:36:39,059 - INFO - Error while receiving.
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\serializers.py", line 437, in dumps
    return cloudpickle.dumps(obj, pickle_protocol)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\cloudpickle\cloudpickle_fast.py", line 72, in dumps
    cp.dump(obj)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\cloudpickle\cloudpickle_fast.py", line 540, in dump
    return Pickler.dump(self, obj)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\context.py", line 353, in __getnewargs__
    raise Exception(
Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\workspace\dragon_storehouse\python_project\sync\a.py", line 191, in run_sync
    self.sync_table_data(table)
  File "D:\workspace\dragon_storehouse\python_project\sync\a.py", line 172, in sync_table_data
    df.foreachPartition(process_partition)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\sql\dataframe.py", line 777, in foreachPartition
    self.rdd.foreachPartition(f)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 937, in foreachPartition
    self.mapPartitions(func).count()  # Force evaluation
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 1235, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 1224, in sum
    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 1078, in fold
    vals = self.mapPartitions(func).collect()
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 949, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 2949, in _jrdd
    wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 2828, in _wrap_function
    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 2814, in _prepare_for_python_RDD
    pickled_command = ser.dumps(command)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\serializers.py", line 447, in dumps
    raise pickle.PicklingError(msg)
_pickle.PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\workspace\dragon_storehouse\python_project\sync\a.py", line 226, in <module>
    syncer.run_sync()
  File "D:\workspace\dragon_storehouse\python_project\sync\a.py", line 200, in run_sync
    time.sleep(10 * retry_count)  # 指数退避
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\context.py", line 285, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1205, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "F:\Auser\anaconda\envs\python_rely\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-08-02 10:36:39,066 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\serializers.py", line 437, in dumps
    return cloudpickle.dumps(obj, pickle_protocol)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\cloudpickle\cloudpickle_fast.py", line 72, in dumps
    cp.dump(obj)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\cloudpickle\cloudpickle_fast.py", line 540, in dump
    return Pickler.dump(self, obj)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\context.py", line 353, in __getnewargs__
    raise Exception(
Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\workspace\dragon_storehouse\python_project\sync\a.py", line 191, in run_sync
    self.sync_table_data(table)
  File "D:\workspace\dragon_storehouse\python_project\sync\a.py", line 172, in sync_table_data
    df.foreachPartition(process_partition)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\sql\dataframe.py", line 777, in foreachPartition
    self.rdd.foreachPartition(f)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 937, in foreachPartition
    self.mapPartitions(func).count()  # Force evaluation
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 1235, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 1224, in sum
    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 1078, in fold
    vals = self.mapPartitions(func).collect()
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 949, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 2949, in _jrdd
    wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 2828, in _wrap_function
    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\rdd.py", line 2814, in _prepare_for_python_RDD
    pickled_command = ser.dumps(command)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\serializers.py", line 447, in dumps
    raise pickle.PicklingError(msg)
_pickle.PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\workspace\dragon_storehouse\python_project\sync\a.py", line 226, in <module>
    syncer.run_sync()
  File "D:\workspace\dragon_storehouse\python_project\sync\a.py", line 200, in run_sync
    time.sleep(10 * retry_count)  # 指数退避
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\pyspark\context.py", line 285, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1205, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "F:\Auser\anaconda\envs\python_rely\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1216, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while receiving
2025-08-02 10:36:39,072 - INFO - 同步任务结束
2025-08-02 10:36:41,144 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:55757)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:36:43,181 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:55757)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:38:07,567 - INFO - 发现 19 张待同步表
2025-08-02 10:38:07,568 - INFO - 开始同步表 ads_page_visit_rank (尝试 1/3)
2025-08-02 10:38:07,868 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:38:09,580 - ERROR - 同步表 ads_page_visit_rank 数据失败: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (Long executor driver): java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

2025-08-02 10:38:09,582 - ERROR - 表 ads_page_visit_rank 同步失败 (尝试 1/3): An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (Long executor driver): java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

2025-08-02 10:38:19,584 - INFO - 开始同步表 ads_page_visit_rank (尝试 2/3)
2025-08-02 10:38:19,738 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:38:19,952 - ERROR - 同步表 ads_page_visit_rank 数据失败: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (Long executor driver): java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

2025-08-02 10:38:19,953 - ERROR - 表 ads_page_visit_rank 同步失败 (尝试 2/3): An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (Long executor driver): java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

2025-08-02 10:39:37,149 - INFO - 发现 19 张待同步表
2025-08-02 10:39:37,149 - INFO - 开始同步表 ads_page_visit_rank (尝试 1/3)
2025-08-02 10:39:37,571 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:39:39,727 - ERROR - 同步表 ads_page_visit_rank 数据失败: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (Long executor driver): java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

2025-08-02 10:39:39,732 - ERROR - 表 ads_page_visit_rank 同步失败 (尝试 1/3): An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (Long executor driver): java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot run program "python3": CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.io.IOException: CreateProcess error=3, 系统找不到指定的路径。
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:453)
	at java.lang.ProcessImpl.start(ProcessImpl.java:140)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 15 more

2025-08-02 10:42:53,284 - INFO - 发现 19 张待同步表
2025-08-02 10:42:53,592 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 10:42:56,672 - INFO - 表 ads_page_visit_rank 数据同步完成，记录数: 1101
2025-08-02 10:42:56,855 - INFO - 表 ads_pc_entry_stats 结构同步成功
2025-08-02 10:45:40,179 - INFO - 表 ads_pc_entry_stats 数据同步完成，记录数: 239695
2025-08-02 10:45:40,381 - INFO - 表 ads_realtime_path_monitor 结构同步成功
2025-08-02 10:45:40,828 - ERROR - 同步表 ads_realtime_path_monitor 数据失败: (1406, "Data too long for column 'next_page_dist' at row 1")
2025-08-02 10:45:40,828 - ERROR - 表 ads_realtime_path_monitor 同步失败，跳过继续处理其他表
2025-08-02 10:45:40,989 - INFO - 表 ads_shop_path_analysis 结构同步成功
2025-08-02 10:45:47,382 - INFO - 表 ads_shop_path_analysis 数据同步完成，记录数: 9267
2025-08-02 10:45:47,546 - INFO - 表 ads_user_segment 结构同步成功
2025-08-02 10:45:47,767 - INFO - 表 ads_user_segment 数据同步完成，记录数: 2
2025-08-02 10:45:47,982 - INFO - 表 ads_wireless_entry_stats 结构同步成功
2025-08-02 10:45:49,103 - INFO - 表 ads_wireless_entry_stats 数据同步完成，记录数: 1100
2025-08-02 10:45:49,340 - INFO - 表 dwd_page_visit_detail 结构同步成功
2025-08-02 10:45:55,419 - ERROR - 同步表 dwd_page_visit_detail 数据失败: An error occurred while calling o102.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 11) (Long executor driver): java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:233)
	at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:53)
	at org.apache.spark.scheduler.DirectTaskResult$$Lambda$2327/76110493.apply$mcV$sp(Unknown Source)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1405)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:606)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:233)
	at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:53)
	at org.apache.spark.scheduler.DirectTaskResult$$Lambda$2327/76110493.apply$mcV$sp(Unknown Source)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1405)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:606)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2025-08-02 10:45:55,424 - ERROR - 表 dwd_page_visit_detail 同步失败，跳过继续处理其他表
2025-08-02 10:45:55,595 - ERROR - 同步表 dwd_user_session 结构失败: (1067, "Invalid default value for 'end_time'")
2025-08-02 10:45:55,596 - ERROR - 表 dwd_user_session 同步失败，跳过继续处理其他表
2025-08-02 10:45:56,141 - INFO - Error while receiving.
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1205, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "F:\Auser\anaconda\envs\python_rely\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-08-02 10:45:56,144 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1205, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "F:\Auser\anaconda\envs\python_rely\lib\socket.py", line 717, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1216, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while receiving
2025-08-02 10:45:56,145 - ERROR - 表 dws_page_path_flow 同步失败，跳过继续处理其他表
2025-08-02 10:45:58,179 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:45:58,180 - ERROR - 表 dws_page_visit_stats 同步失败，跳过继续处理其他表
2025-08-02 10:46:00,219 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:00,220 - ERROR - 表 dws_traffic_entry_stats 同步失败，跳过继续处理其他表
2025-08-02 10:46:02,259 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:02,260 - ERROR - 表 ods_marketing_activity 同步失败，跳过继续处理其他表
2025-08-02 10:46:04,293 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:04,294 - ERROR - 表 ods_order_info 同步失败，跳过继续处理其他表
2025-08-02 10:46:06,330 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:06,330 - ERROR - 表 ods_page_relationship 同步失败，跳过继续处理其他表
2025-08-02 10:46:08,371 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:08,372 - ERROR - 表 ods_page_visit_log 同步失败，跳过继续处理其他表
2025-08-02 10:46:10,413 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:10,414 - ERROR - 表 ods_product_info 同步失败，跳过继续处理其他表
2025-08-02 10:46:12,451 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:12,452 - ERROR - 表 ods_shop_page_info 同步失败，跳过继续处理其他表
2025-08-02 10:46:14,493 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:14,495 - ERROR - 表 ods_traffic_source 同步失败，跳过继续处理其他表
2025-08-02 10:46:16,529 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:16,530 - ERROR - 表 ods_user_info 同步失败，跳过继续处理其他表
2025-08-02 10:46:18,569 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:21,009 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:23,065 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:25,102 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:27,143 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:29,183 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:31,222 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:33,263 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:35,302 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:37,343 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:39,383 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:41,422 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 10:46:43,459 - ERROR - An error occurred while trying to connect to the Java server (127.0.0.1:57140)
Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 982, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\Auser\anaconda\envs\python_rely\lib\site-packages\py4j\java_gateway.py", line 1120, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-08-02 11:03:30,542 - INFO - 发现 19 张待同步表
2025-08-02 11:03:30,543 - INFO - 开始同步表 ads_page_visit_rank
2025-08-02 11:03:30,869 - INFO - 表 ads_page_visit_rank 结构同步成功
2025-08-02 11:03:33,369 - INFO - 表 ads_page_visit_rank 数据同步完成，记录数: 1101
2025-08-02 11:03:33,370 - INFO - 表 ads_page_visit_rank 同步成功
2025-08-02 11:03:33,370 - INFO - 开始同步表 ads_pc_entry_stats
2025-08-02 11:03:33,601 - INFO - 表 ads_pc_entry_stats 结构同步成功
2025-08-02 11:03:39,424 - INFO - 表 ads_pc_entry_stats 数据同步完成，记录数: 239695
2025-08-02 11:03:39,453 - INFO - 表 ads_pc_entry_stats 同步成功
2025-08-02 11:03:39,453 - INFO - 开始同步表 ads_realtime_path_monitor
2025-08-02 11:03:39,619 - INFO - 表 ads_realtime_path_monitor 结构同步成功
2025-08-02 11:03:40,307 - INFO - 表 ads_realtime_path_monitor 数据同步完成，记录数: 1101
2025-08-02 11:03:40,307 - INFO - 表 ads_realtime_path_monitor 同步成功
2025-08-02 11:03:40,307 - INFO - 开始同步表 ads_shop_path_analysis
2025-08-02 11:03:40,467 - INFO - 表 ads_shop_path_analysis 结构同步成功
2025-08-02 11:03:41,074 - INFO - 表 ads_shop_path_analysis 数据同步完成，记录数: 9267
2025-08-02 11:03:41,076 - INFO - 表 ads_shop_path_analysis 同步成功
2025-08-02 11:03:41,076 - INFO - 开始同步表 ads_user_segment
2025-08-02 11:03:41,240 - INFO - 表 ads_user_segment 结构同步成功
2025-08-02 11:03:41,410 - INFO - 表 ads_user_segment 数据同步完成，记录数: 2
2025-08-02 11:03:41,410 - INFO - 表 ads_user_segment 同步成功
2025-08-02 11:03:41,410 - INFO - 开始同步表 ads_wireless_entry_stats
2025-08-02 11:03:41,584 - INFO - 表 ads_wireless_entry_stats 结构同步成功
2025-08-02 11:03:41,884 - INFO - 表 ads_wireless_entry_stats 数据同步完成，记录数: 1100
2025-08-02 11:03:41,884 - INFO - 表 ads_wireless_entry_stats 同步成功
2025-08-02 11:03:41,884 - INFO - 开始同步表 dwd_page_visit_detail
2025-08-02 11:03:42,086 - INFO - 表 dwd_page_visit_detail 结构同步成功
2025-08-02 11:03:42,622 - ERROR - 同步大表 dwd_page_visit_detail 数据失败: 
mismatched input 'OFFSET' expecting {<EOF>, ';'}(line 3, pos 32)

== SQL ==

                    SELECT * FROM gmall_09.dwd_page_visit_detail 
                    LIMIT 50000 OFFSET 0
--------------------------------^^^
                

2025-08-02 11:03:42,626 - ERROR - 同步表 dwd_page_visit_detail 数据失败: 
mismatched input 'OFFSET' expecting {<EOF>, ';'}(line 3, pos 32)

== SQL ==

                    SELECT * FROM gmall_09.dwd_page_visit_detail 
                    LIMIT 50000 OFFSET 0
--------------------------------^^^
                

2025-08-02 11:03:42,626 - ERROR - 表 dwd_page_visit_detail 同步失败，跳过继续处理其他表
2025-08-02 11:03:42,626 - INFO - 开始同步表 dwd_user_session
2025-08-02 11:03:42,765 - ERROR - 同步表 dwd_user_session 结构失败: (1067, "Invalid default value for 'end_time'")
2025-08-02 11:03:42,766 - ERROR - 表 dwd_user_session 同步失败，跳过继续处理其他表
2025-08-02 11:03:42,766 - INFO - 开始同步表 dws_page_path_flow
2025-08-02 11:03:42,932 - INFO - 表 dws_page_path_flow 结构同步成功
2025-08-02 11:03:45,915 - INFO - 表 dws_page_path_flow 数据同步完成，记录数: 83828
2025-08-02 11:03:45,934 - INFO - 表 dws_page_path_flow 同步成功
2025-08-02 11:03:45,934 - INFO - 开始同步表 dws_page_visit_stats
2025-08-02 11:03:46,107 - INFO - 表 dws_page_visit_stats 结构同步成功
2025-08-02 11:03:46,373 - INFO - 表 dws_page_visit_stats 数据同步完成，记录数: 1101
2025-08-02 11:03:46,374 - INFO - 表 dws_page_visit_stats 同步成功
2025-08-02 11:03:46,374 - INFO - 开始同步表 dws_traffic_entry_stats
2025-08-02 11:03:46,538 - INFO - 表 dws_traffic_entry_stats 结构同步成功
2025-08-02 11:03:46,762 - INFO - 表 dws_traffic_entry_stats 数据同步完成，记录数: 1102
2025-08-02 11:03:46,762 - INFO - 表 dws_traffic_entry_stats 同步成功
2025-08-02 11:03:46,762 - INFO - 开始同步表 ods_marketing_activity
2025-08-02 11:03:46,916 - ERROR - 同步表 ods_marketing_activity 结构失败: (1067, "Invalid default value for 'end_time'")
2025-08-02 11:03:46,916 - ERROR - 表 ods_marketing_activity 同步失败，跳过继续处理其他表
2025-08-02 11:03:46,916 - INFO - 开始同步表 ods_order_info
2025-08-02 11:03:47,090 - ERROR - 同步表 ods_order_info 结构失败: (1067, "Invalid default value for 'payment_time'")
2025-08-02 11:03:47,090 - ERROR - 表 ods_order_info 同步失败，跳过继续处理其他表
2025-08-02 11:03:47,090 - INFO - 开始同步表 ods_page_relationship
2025-08-02 11:03:47,248 - INFO - 表 ods_page_relationship 结构同步成功
2025-08-02 11:03:47,966 - INFO - 表 ods_page_relationship 数据同步完成，记录数: 200
2025-08-02 11:03:47,966 - INFO - 表 ods_page_relationship 同步成功
2025-08-02 11:03:47,966 - INFO - 开始同步表 ods_page_visit_log
2025-08-02 11:03:48,129 - INFO - 表 ods_page_visit_log 结构同步成功
2025-08-02 11:04:45,304 - INFO - 表 ods_page_visit_log 数据同步完成，记录数: 1000000
2025-08-02 11:04:45,658 - INFO - 表 ods_page_visit_log 同步成功
2025-08-02 11:04:45,658 - INFO - 开始同步表 ods_product_info
2025-08-02 11:04:45,950 - INFO - 表 ods_product_info 结构同步成功
2025-08-02 11:04:46,229 - INFO - 表 ods_product_info 数据同步完成，记录数: 1000
2025-08-02 11:04:46,229 - INFO - 表 ods_product_info 同步成功
2025-08-02 11:04:46,229 - INFO - 开始同步表 ods_shop_page_info
2025-08-02 11:04:46,378 - INFO - 表 ods_shop_page_info 结构同步成功
2025-08-02 11:04:46,556 - INFO - 表 ods_shop_page_info 数据同步完成，记录数: 100
2025-08-02 11:04:46,557 - INFO - 表 ods_shop_page_info 同步成功
2025-08-02 11:04:46,557 - INFO - 开始同步表 ods_traffic_source
2025-08-02 11:04:46,719 - INFO - 表 ods_traffic_source 结构同步成功
2025-08-02 11:04:46,888 - INFO - 表 ods_traffic_source 数据同步完成，记录数: 20
2025-08-02 11:04:46,888 - INFO - 表 ods_traffic_source 同步成功
2025-08-02 11:04:46,888 - INFO - 开始同步表 ods_user_info
2025-08-02 11:04:47,045 - INFO - 表 ods_user_info 结构同步成功
2025-08-02 11:04:47,585 - INFO - 表 ods_user_info 数据同步完成，记录数: 10000
2025-08-02 11:04:47,587 - INFO - 表 ods_user_info 同步成功
2025-08-07 16:46:35,208 - INFO - 发现 26 张待同步表
2025-08-07 16:46:35,209 - INFO - 开始同步表 ads_category_ranking
2025-08-07 16:46:35,512 - ERROR - 同步表 ads_category_ranking 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:35,512 - ERROR - 表 ads_category_ranking 同步失败，跳过继续处理其他表
2025-08-07 16:46:35,512 - INFO - 开始同步表 ads_data_validation
2025-08-07 16:46:35,666 - ERROR - 同步表 ads_data_validation 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:35,666 - ERROR - 表 ads_data_validation 同步失败，跳过继续处理其他表
2025-08-07 16:46:35,666 - INFO - 开始同步表 ads_product_interval_analysis
2025-08-07 16:46:35,874 - ERROR - 同步表 ads_product_interval_analysis 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:35,874 - ERROR - 表 ads_product_interval_analysis 同步失败，跳过继续处理其他表
2025-08-07 16:46:35,874 - INFO - 开始同步表 ads_product_macro_monitor
2025-08-07 16:46:36,055 - ERROR - 同步表 ads_product_macro_monitor 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:36,055 - ERROR - 表 ads_product_macro_monitor 同步失败，跳过继续处理其他表
2025-08-07 16:46:36,055 - INFO - 开始同步表 ads_product_ranking
2025-08-07 16:46:36,211 - ERROR - 同步表 ads_product_ranking 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:36,211 - ERROR - 表 ads_product_ranking 同步失败，跳过继续处理其他表
2025-08-07 16:46:36,212 - INFO - 开始同步表 dwd_micro_detail_behavior
2025-08-07 16:46:36,393 - ERROR - 同步表 dwd_micro_detail_behavior 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:36,393 - ERROR - 表 dwd_micro_detail_behavior 同步失败，跳过继续处理其他表
2025-08-07 16:46:36,393 - INFO - 开始同步表 dwd_product_behavior_detail
2025-08-07 16:46:36,524 - ERROR - 同步表 dwd_product_behavior_detail 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:36,524 - ERROR - 表 dwd_product_behavior_detail 同步失败，跳过继续处理其他表
2025-08-07 16:46:36,524 - INFO - 开始同步表 dwd_product_daily_summary
2025-08-07 16:46:36,702 - ERROR - 同步表 dwd_product_daily_summary 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:36,702 - ERROR - 表 dwd_product_daily_summary 同步失败，跳过继续处理其他表
2025-08-07 16:46:36,702 - INFO - 开始同步表 dwd_product_stratification
2025-08-07 16:46:36,857 - ERROR - 同步表 dwd_product_stratification 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:36,857 - ERROR - 表 dwd_product_stratification 同步失败，跳过继续处理其他表
2025-08-07 16:46:36,857 - INFO - 开始同步表 dwd_product_trade_detail
2025-08-07 16:46:37,054 - ERROR - 同步表 dwd_product_trade_detail 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:37,054 - ERROR - 表 dwd_product_trade_detail 同步失败，跳过继续处理其他表
2025-08-07 16:46:37,054 - INFO - 开始同步表 dws_category_day_summary
2025-08-07 16:46:37,272 - ERROR - 同步表 dws_category_day_summary 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:37,272 - ERROR - 表 dws_category_day_summary 同步失败，跳过继续处理其他表
2025-08-07 16:46:37,273 - INFO - 开始同步表 dws_product_day_summary
2025-08-07 16:46:37,448 - ERROR - 同步表 dws_product_day_summary 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:37,448 - ERROR - 表 dws_product_day_summary 同步失败，跳过继续处理其他表
2025-08-07 16:46:37,448 - INFO - 开始同步表 dws_product_month_summary
2025-08-07 16:46:37,632 - ERROR - 同步表 dws_product_month_summary 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:37,632 - ERROR - 表 dws_product_month_summary 同步失败，跳过继续处理其他表
2025-08-07 16:46:37,632 - INFO - 开始同步表 dws_product_week_summary
2025-08-07 16:46:37,885 - ERROR - 同步表 dws_product_week_summary 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:37,885 - ERROR - 表 dws_product_week_summary 同步失败，跳过继续处理其他表
2025-08-07 16:46:37,885 - INFO - 开始同步表 ods_marketing_activity
2025-08-07 16:46:38,062 - ERROR - 同步表 ods_marketing_activity 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:38,062 - ERROR - 表 ods_marketing_activity 同步失败，跳过继续处理其他表
2025-08-07 16:46:38,062 - INFO - 开始同步表 ods_micro_detail_visit
2025-08-07 16:46:38,221 - ERROR - 同步表 ods_micro_detail_visit 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:38,221 - ERROR - 表 ods_micro_detail_visit 同步失败，跳过继续处理其他表
2025-08-07 16:46:38,221 - INFO - 开始同步表 ods_order_detail
2025-08-07 16:46:38,387 - ERROR - 同步表 ods_order_detail 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:38,387 - ERROR - 表 ods_order_detail 同步失败，跳过继续处理其他表
2025-08-07 16:46:38,387 - INFO - 开始同步表 ods_order_info
2025-08-07 16:46:38,538 - ERROR - 同步表 ods_order_info 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:38,539 - ERROR - 表 ods_order_info 同步失败，跳过继续处理其他表
2025-08-07 16:46:38,539 - INFO - 开始同步表 ods_order_refund
2025-08-07 16:46:38,679 - ERROR - 同步表 ods_order_refund 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:38,680 - ERROR - 表 ods_order_refund 同步失败，跳过继续处理其他表
2025-08-07 16:46:38,680 - INFO - 开始同步表 ods_product_category
2025-08-07 16:46:38,863 - ERROR - 同步表 ods_product_category 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:38,864 - ERROR - 表 ods_product_category 同步失败，跳过继续处理其他表
2025-08-07 16:46:38,864 - INFO - 开始同步表 ods_product_competitiveness
2025-08-07 16:46:39,006 - ERROR - 同步表 ods_product_competitiveness 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:39,006 - ERROR - 表 ods_product_competitiveness 同步失败，跳过继续处理其他表
2025-08-07 16:46:39,006 - INFO - 开始同步表 ods_product_favorite
2025-08-07 16:46:39,171 - ERROR - 同步表 ods_product_favorite 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:39,171 - ERROR - 表 ods_product_favorite 同步失败，跳过继续处理其他表
2025-08-07 16:46:39,171 - INFO - 开始同步表 ods_product_info
2025-08-07 16:46:39,327 - ERROR - 同步表 ods_product_info 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:39,327 - ERROR - 表 ods_product_info 同步失败，跳过继续处理其他表
2025-08-07 16:46:39,327 - INFO - 开始同步表 ods_shopping_cart
2025-08-07 16:46:39,513 - ERROR - 同步表 ods_shopping_cart 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:39,513 - ERROR - 表 ods_shopping_cart 同步失败，跳过继续处理其他表
2025-08-07 16:46:39,513 - INFO - 开始同步表 ods_user_behavior
2025-08-07 16:46:39,714 - ERROR - 同步表 ods_user_behavior 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:39,715 - ERROR - 表 ods_user_behavior 同步失败，跳过继续处理其他表
2025-08-07 16:46:39,715 - INFO - 开始同步表 ods_yearly_payment_snapshot
2025-08-07 16:46:39,884 - ERROR - 同步表 ods_yearly_payment_snapshot 结构失败: (1049, "Unknown database 'gmall_01_ads'")
2025-08-07 16:46:39,885 - ERROR - 表 ods_yearly_payment_snapshot 同步失败，跳过继续处理其他表
2025-08-07 16:47:22,900 - INFO - 发现 26 张待同步表
2025-08-07 16:47:22,900 - INFO - 开始同步表 ads_category_ranking
2025-08-07 16:47:23,210 - INFO - 表 ads_category_ranking 结构同步成功
2025-08-07 16:47:25,983 - INFO - 表 ads_category_ranking 数据同步完成，记录数: 48
2025-08-07 16:47:25,983 - INFO - 表 ads_category_ranking 同步成功
2025-08-07 16:47:25,983 - INFO - 开始同步表 ads_data_validation
2025-08-07 16:47:26,154 - INFO - 表 ads_data_validation 结构同步成功
2025-08-07 16:47:26,446 - INFO - 表 ads_data_validation 数据同步完成，记录数: 9
2025-08-07 16:47:26,446 - INFO - 表 ads_data_validation 同步成功
2025-08-07 16:47:26,446 - INFO - 开始同步表 ads_product_interval_analysis
2025-08-07 16:47:26,629 - INFO - 表 ads_product_interval_analysis 结构同步成功
2025-08-07 16:47:27,309 - INFO - 表 ads_product_interval_analysis 数据同步完成，记录数: 332
2025-08-07 16:47:27,309 - INFO - 表 ads_product_interval_analysis 同步成功
2025-08-07 16:47:27,309 - INFO - 开始同步表 ads_product_macro_monitor
2025-08-07 16:47:27,464 - INFO - 表 ads_product_macro_monitor 结构同步成功
2025-08-07 16:47:27,921 - INFO - 表 ads_product_macro_monitor 数据同步完成，记录数: 48
2025-08-07 16:47:27,921 - INFO - 表 ads_product_macro_monitor 同步成功
2025-08-07 16:47:27,921 - INFO - 开始同步表 ads_product_ranking
2025-08-07 16:47:28,289 - INFO - 表 ads_product_ranking 结构同步成功
2025-08-07 16:47:28,670 - INFO - 表 ads_product_ranking 数据同步完成，记录数: 48
2025-08-07 16:47:28,671 - INFO - 表 ads_product_ranking 同步成功
2025-08-07 16:47:28,671 - INFO - 开始同步表 dwd_micro_detail_behavior
2025-08-07 16:47:29,053 - INFO - 表 dwd_micro_detail_behavior 结构同步成功
2025-08-07 16:47:29,425 - INFO - 表 dwd_micro_detail_behavior 数据同步完成，记录数: 3750
2025-08-07 16:47:29,425 - INFO - 表 dwd_micro_detail_behavior 同步成功
2025-08-07 16:47:29,425 - INFO - 开始同步表 dwd_product_behavior_detail
2025-08-07 16:47:29,593 - INFO - 表 dwd_product_behavior_detail 结构同步成功
2025-08-07 16:47:30,923 - INFO - 表 dwd_product_behavior_detail 数据同步完成，记录数: 20051
2025-08-07 16:47:30,926 - INFO - 表 dwd_product_behavior_detail 同步成功
2025-08-07 16:47:30,927 - INFO - 开始同步表 dwd_product_daily_summary
2025-08-07 16:47:31,106 - INFO - 表 dwd_product_daily_summary 结构同步成功
2025-08-07 16:47:31,401 - INFO - 表 dwd_product_daily_summary 数据同步完成，记录数: 1500
2025-08-07 16:47:31,402 - INFO - 表 dwd_product_daily_summary 同步成功
2025-08-07 16:47:31,402 - INFO - 开始同步表 dwd_product_stratification
2025-08-07 16:47:31,567 - INFO - 表 dwd_product_stratification 结构同步成功
2025-08-07 16:47:31,841 - INFO - 表 dwd_product_stratification 数据同步完成，记录数: 1500
2025-08-07 16:47:31,842 - INFO - 表 dwd_product_stratification 同步成功
2025-08-07 16:47:31,842 - INFO - 开始同步表 dwd_product_trade_detail
2025-08-07 16:47:32,006 - ERROR - 同步表 dwd_product_trade_detail 结构失败: (1067, "Invalid default value for 'payment_time'")
2025-08-07 16:47:32,007 - ERROR - 表 dwd_product_trade_detail 同步失败，跳过继续处理其他表
2025-08-07 16:47:32,007 - INFO - 开始同步表 dws_category_day_summary
2025-08-07 16:47:32,167 - INFO - 表 dws_category_day_summary 结构同步成功
2025-08-07 16:47:32,385 - INFO - 表 dws_category_day_summary 数据同步完成，记录数: 48
2025-08-07 16:47:32,385 - INFO - 表 dws_category_day_summary 同步成功
2025-08-07 16:47:32,385 - INFO - 开始同步表 dws_product_day_summary
2025-08-07 16:47:32,537 - INFO - 表 dws_product_day_summary 结构同步成功
2025-08-07 16:47:32,764 - INFO - 表 dws_product_day_summary 数据同步完成，记录数: 48
2025-08-07 16:47:32,764 - INFO - 表 dws_product_day_summary 同步成功
2025-08-07 16:47:32,764 - INFO - 开始同步表 dws_product_month_summary
2025-08-07 16:47:32,969 - INFO - 表 dws_product_month_summary 结构同步成功
2025-08-07 16:47:33,178 - ERROR - 同步表 dws_product_month_summary 数据失败: An error occurred while calling o153.collectToPython.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://cdh01:8020/warehouse/gmall_01/dws/dws_product_day_summary/bdp_month=$%7Byear_month}
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)
	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:85)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)
	at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2025-08-07 16:47:33,181 - ERROR - 同步表 dws_product_month_summary 数据失败: An error occurred while calling o153.collectToPython.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://cdh01:8020/warehouse/gmall_01/dws/dws_product_day_summary/bdp_month=$%7Byear_month}
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)
	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:85)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)
	at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2025-08-07 16:47:33,182 - ERROR - 表 dws_product_month_summary 同步失败，跳过继续处理其他表
2025-08-07 16:47:33,182 - INFO - 开始同步表 dws_product_week_summary
2025-08-07 16:47:33,360 - INFO - 表 dws_product_week_summary 结构同步成功
2025-08-07 16:47:33,636 - ERROR - 同步表 dws_product_week_summary 数据失败: An error occurred while calling o160.collectToPython.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://cdh01:8020/warehouse/gmall_01/dws/dws_product_day_summary/bdp_year_week=$%7Byear_week}
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)
	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:85)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)
	at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2025-08-07 16:47:33,638 - ERROR - 同步表 dws_product_week_summary 数据失败: An error occurred while calling o160.collectToPython.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://cdh01:8020/warehouse/gmall_01/dws/dws_product_day_summary/bdp_year_week=$%7Byear_week}
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)
	at org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:85)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)
	at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2025-08-07 16:47:33,638 - ERROR - 表 dws_product_week_summary 同步失败，跳过继续处理其他表
2025-08-07 16:47:33,638 - INFO - 开始同步表 ods_marketing_activity
2025-08-07 16:47:33,828 - INFO - 表 ods_marketing_activity 结构同步成功
2025-08-07 16:47:34,046 - INFO - 表 ods_marketing_activity 数据同步完成，记录数: 262
2025-08-07 16:47:34,046 - INFO - 表 ods_marketing_activity 同步成功
2025-08-07 16:47:34,046 - INFO - 开始同步表 ods_micro_detail_visit
2025-08-07 16:47:34,215 - INFO - 表 ods_micro_detail_visit 结构同步成功
2025-08-07 16:47:34,501 - INFO - 表 ods_micro_detail_visit 数据同步完成，记录数: 3750
2025-08-07 16:47:34,501 - INFO - 表 ods_micro_detail_visit 同步成功
2025-08-07 16:47:34,501 - INFO - 开始同步表 ods_order_detail
2025-08-07 16:47:34,659 - INFO - 表 ods_order_detail 结构同步成功
2025-08-07 16:47:35,003 - INFO - 表 ods_order_detail 数据同步完成，记录数: 3480
2025-08-07 16:47:35,004 - INFO - 表 ods_order_detail 同步成功
2025-08-07 16:47:35,004 - INFO - 开始同步表 ods_order_info
2025-08-07 16:47:35,201 - INFO - 表 ods_order_info 结构同步成功
2025-08-07 16:47:35,450 - INFO - 表 ods_order_info 数据同步完成，记录数: 1500
2025-08-07 16:47:35,450 - INFO - 表 ods_order_info 同步成功
2025-08-07 16:47:35,450 - INFO - 开始同步表 ods_order_refund
2025-08-07 16:47:35,617 - INFO - 表 ods_order_refund 结构同步成功
2025-08-07 16:47:35,786 - INFO - 表 ods_order_refund 数据同步完成，记录数: 72
2025-08-07 16:47:35,786 - INFO - 表 ods_order_refund 同步成功
2025-08-07 16:47:35,786 - INFO - 开始同步表 ods_product_category
2025-08-07 16:47:35,944 - INFO - 表 ods_product_category 结构同步成功
2025-08-07 16:47:36,141 - INFO - 表 ods_product_category 数据同步完成，记录数: 48
2025-08-07 16:47:36,141 - INFO - 表 ods_product_category 同步成功
2025-08-07 16:47:36,141 - INFO - 开始同步表 ods_product_competitiveness
2025-08-07 16:47:36,285 - INFO - 表 ods_product_competitiveness 结构同步成功
2025-08-07 16:47:36,505 - INFO - 表 ods_product_competitiveness 数据同步完成，记录数: 1500
2025-08-07 16:47:36,506 - INFO - 表 ods_product_competitiveness 同步成功
2025-08-07 16:47:36,506 - INFO - 开始同步表 ods_product_favorite
2025-08-07 16:47:36,649 - INFO - 表 ods_product_favorite 结构同步成功
2025-08-07 16:47:36,857 - INFO - 表 ods_product_favorite 数据同步完成，记录数: 750
2025-08-07 16:47:36,857 - INFO - 表 ods_product_favorite 同步成功
2025-08-07 16:47:36,857 - INFO - 开始同步表 ods_product_info
2025-08-07 16:47:37,006 - INFO - 表 ods_product_info 结构同步成功
2025-08-07 16:47:37,247 - INFO - 表 ods_product_info 数据同步完成，记录数: 1500
2025-08-07 16:47:37,247 - INFO - 表 ods_product_info 同步成功
2025-08-07 16:47:37,247 - INFO - 开始同步表 ods_shopping_cart
2025-08-07 16:47:37,396 - INFO - 表 ods_shopping_cart 结构同步成功
2025-08-07 16:47:37,612 - INFO - 表 ods_shopping_cart 数据同步完成，记录数: 1125
2025-08-07 16:47:37,612 - INFO - 表 ods_shopping_cart 同步成功
2025-08-07 16:47:37,612 - INFO - 开始同步表 ods_user_behavior
2025-08-07 16:47:37,819 - INFO - 表 ods_user_behavior 结构同步成功
2025-08-07 16:47:39,199 - INFO - 表 ods_user_behavior 数据同步完成，记录数: 30000
2025-08-07 16:47:39,205 - INFO - 表 ods_user_behavior 同步成功
2025-08-07 16:47:39,205 - INFO - 开始同步表 ods_yearly_payment_snapshot
2025-08-07 16:47:39,352 - INFO - 表 ods_yearly_payment_snapshot 结构同步成功
2025-08-07 16:47:39,581 - INFO - 表 ods_yearly_payment_snapshot 数据同步完成，记录数: 1500
2025-08-07 16:47:39,582 - INFO - 表 ods_yearly_payment_snapshot 同步成功
2025-08-07 16:48:22,942 - INFO - 发现 37 张待同步表
2025-08-07 16:48:22,943 - INFO - 开始同步表 ads_category_ranking
2025-08-07 16:48:23,276 - INFO - 表 ads_category_ranking 结构同步成功
2025-08-07 16:48:25,829 - INFO - 表 ads_category_ranking 数据同步完成，记录数: 36
2025-08-07 16:48:25,829 - INFO - 表 ads_category_ranking 同步成功
2025-08-07 16:48:25,830 - INFO - 开始同步表 ads_follow_product_analysis
2025-08-07 16:48:26,014 - INFO - 表 ads_follow_product_analysis 结构同步成功
2025-08-07 16:48:26,774 - INFO - 表 ads_follow_product_analysis 数据同步完成，记录数: 1956
2025-08-07 16:48:26,775 - INFO - 表 ads_follow_product_analysis 同步成功
2025-08-07 16:48:26,775 - INFO - 开始同步表 ads_my_follow_product_analysis
2025-08-07 16:48:26,938 - INFO - 表 ads_my_follow_product_analysis 结构同步成功
2025-08-07 16:48:27,681 - INFO - 表 ads_my_follow_product_analysis 数据同步完成，记录数: 8000
2025-08-07 16:48:27,684 - INFO - 表 ads_my_follow_product_analysis 同步成功
2025-08-07 16:48:27,684 - INFO - 开始同步表 ads_price_power_analysis
2025-08-07 16:48:27,836 - INFO - 表 ads_price_power_analysis 结构同步成功
2025-08-07 16:48:28,151 - INFO - 表 ads_price_power_analysis 数据同步完成，记录数: 565
2025-08-07 16:48:28,152 - INFO - 表 ads_price_power_analysis 同步成功
2025-08-07 16:48:28,152 - INFO - 开始同步表 ads_price_power_overview
2025-08-07 16:48:28,334 - INFO - 表 ads_price_power_overview 结构同步成功
2025-08-07 16:48:28,564 - INFO - 表 ads_price_power_overview 数据同步完成，记录数: 1
2025-08-07 16:48:28,564 - INFO - 表 ads_price_power_overview 同步成功
2025-08-07 16:48:28,564 - INFO - 开始同步表 ads_product_alert_summary
2025-08-07 16:48:28,696 - INFO - 表 ads_product_alert_summary 结构同步成功
2025-08-07 16:48:28,918 - INFO - 表 ads_product_alert_summary 数据同步完成，记录数: 235
2025-08-07 16:48:28,919 - INFO - 表 ads_product_alert_summary 同步成功
2025-08-07 16:48:28,919 - INFO - 开始同步表 ads_product_ranking
2025-08-07 16:48:29,124 - INFO - 表 ads_product_ranking 结构同步成功
2025-08-07 16:48:29,589 - INFO - 表 ads_product_ranking 数据同步完成，记录数: 2000
2025-08-07 16:48:29,590 - INFO - 表 ads_product_ranking 同步成功
2025-08-07 16:48:29,591 - INFO - 开始同步表 ads_search_keyword_top10
2025-08-07 16:48:29,776 - INFO - 表 ads_search_keyword_top10 结构同步成功
2025-08-07 16:48:30,007 - INFO - 表 ads_search_keyword_top10 数据同步完成，记录数: 10
2025-08-07 16:48:30,007 - INFO - 表 ads_search_keyword_top10 同步成功
2025-08-07 16:48:30,007 - INFO - 开始同步表 ads_sku_sales_top5
2025-08-07 16:48:30,164 - INFO - 表 ads_sku_sales_top5 结构同步成功
2025-08-07 16:48:30,471 - INFO - 表 ads_sku_sales_top5 数据同步完成，记录数: 1439
2025-08-07 16:48:30,472 - INFO - 表 ads_sku_sales_top5 同步成功
2025-08-07 16:48:30,472 - INFO - 开始同步表 ads_traffic_source_top10
2025-08-07 16:48:30,639 - INFO - 表 ads_traffic_source_top10 结构同步成功
2025-08-07 16:48:30,934 - INFO - 表 ads_traffic_source_top10 数据同步完成，记录数: 2000
2025-08-07 16:48:30,935 - INFO - 表 ads_traffic_source_top10 同步成功
2025-08-07 16:48:30,935 - INFO - 开始同步表 dwd_category_sales_detail
2025-08-07 16:48:31,087 - INFO - 表 dwd_category_sales_detail 结构同步成功
2025-08-07 16:48:31,348 - INFO - 表 dwd_category_sales_detail 数据同步完成，记录数: 36
2025-08-07 16:48:31,348 - INFO - 表 dwd_category_sales_detail 同步成功
2025-08-07 16:48:31,348 - INFO - 开始同步表 dwd_price_power_detail
2025-08-07 16:48:31,534 - INFO - 表 dwd_price_power_detail 结构同步成功
2025-08-07 16:48:31,868 - INFO - 表 dwd_price_power_detail 数据同步完成，记录数: 500
2025-08-07 16:48:31,884 - INFO - 表 dwd_price_power_detail 同步成功
2025-08-07 16:48:31,885 - INFO - 开始同步表 dwd_product_alert_detail
2025-08-07 16:48:32,193 - INFO - 表 dwd_product_alert_detail 结构同步成功
2025-08-07 16:48:32,489 - INFO - 表 dwd_product_alert_detail 数据同步完成，记录数: 300
2025-08-07 16:48:32,489 - INFO - 表 dwd_product_alert_detail 同步成功
2025-08-07 16:48:32,489 - INFO - 开始同步表 dwd_product_follow_detail
2025-08-07 16:48:32,666 - INFO - 表 dwd_product_follow_detail 结构同步成功
2025-08-07 16:48:32,939 - INFO - 表 dwd_product_follow_detail 数据同步完成，记录数: 2000
2025-08-07 16:48:32,940 - INFO - 表 dwd_product_follow_detail 同步成功
2025-08-07 16:48:32,940 - INFO - 开始同步表 dwd_product_sales_detail
2025-08-07 16:48:33,111 - INFO - 表 dwd_product_sales_detail 结构同步成功
2025-08-07 16:48:33,675 - INFO - 表 dwd_product_sales_detail 数据同步完成，记录数: 5000
2025-08-07 16:48:33,678 - INFO - 表 dwd_product_sales_detail 同步成功
2025-08-07 16:48:33,678 - INFO - 开始同步表 dwd_product_traffic_detail
2025-08-07 16:48:33,840 - INFO - 表 dwd_product_traffic_detail 结构同步成功
2025-08-07 16:48:34,138 - INFO - 表 dwd_product_traffic_detail 数据同步完成，记录数: 2000
2025-08-07 16:48:34,139 - INFO - 表 dwd_product_traffic_detail 同步成功
2025-08-07 16:48:34,139 - INFO - 开始同步表 dwd_search_keyword_detail
2025-08-07 16:48:34,304 - INFO - 表 dwd_search_keyword_detail 结构同步成功
2025-08-07 16:48:34,521 - INFO - 表 dwd_search_keyword_detail 数据同步完成，记录数: 1500
2025-08-07 16:48:34,521 - INFO - 表 dwd_search_keyword_detail 同步成功
2025-08-07 16:48:34,521 - INFO - 开始同步表 dwd_sku_sales_detail
2025-08-07 16:48:34,654 - INFO - 表 dwd_sku_sales_detail 结构同步成功
2025-08-07 16:48:34,914 - INFO - 表 dwd_sku_sales_detail 数据同步完成，记录数: 1439
2025-08-07 16:48:34,914 - INFO - 表 dwd_sku_sales_detail 同步成功
2025-08-07 16:48:34,914 - INFO - 开始同步表 dws_category_sales_d
2025-08-07 16:48:35,092 - INFO - 表 dws_category_sales_d 结构同步成功
2025-08-07 16:48:35,291 - INFO - 表 dws_category_sales_d 数据同步完成，记录数: 36
2025-08-07 16:48:35,291 - INFO - 表 dws_category_sales_d 同步成功
2025-08-07 16:48:35,291 - INFO - 开始同步表 dws_price_power_d
2025-08-07 16:48:35,435 - INFO - 表 dws_price_power_d 结构同步成功
2025-08-07 16:48:35,676 - INFO - 表 dws_price_power_d 数据同步完成，记录数: 500
2025-08-07 16:48:35,676 - INFO - 表 dws_price_power_d 同步成功
2025-08-07 16:48:35,677 - INFO - 开始同步表 dws_product_alert_d
2025-08-07 16:48:35,846 - INFO - 表 dws_product_alert_d 结构同步成功
2025-08-07 16:48:36,039 - INFO - 表 dws_product_alert_d 数据同步完成，记录数: 235
2025-08-07 16:48:36,039 - INFO - 表 dws_product_alert_d 同步成功
2025-08-07 16:48:36,039 - INFO - 开始同步表 dws_product_follow_d
2025-08-07 16:48:36,201 - INFO - 表 dws_product_follow_d 结构同步成功
2025-08-07 16:48:36,536 - INFO - 表 dws_product_follow_d 数据同步完成，记录数: 489
2025-08-07 16:48:36,537 - INFO - 表 dws_product_follow_d 同步成功
2025-08-07 16:48:36,537 - INFO - 开始同步表 dws_product_sales_d
2025-08-07 16:48:36,694 - INFO - 表 dws_product_sales_d 结构同步成功
2025-08-07 16:48:37,010 - INFO - 表 dws_product_sales_d 数据同步完成，记录数: 500
2025-08-07 16:48:37,011 - INFO - 表 dws_product_sales_d 同步成功
2025-08-07 16:48:37,011 - INFO - 开始同步表 dws_product_traffic_d
2025-08-07 16:48:37,192 - INFO - 表 dws_product_traffic_d 结构同步成功
2025-08-07 16:48:37,535 - INFO - 表 dws_product_traffic_d 数据同步完成，记录数: 2000
2025-08-07 16:48:37,536 - INFO - 表 dws_product_traffic_d 同步成功
2025-08-07 16:48:37,536 - INFO - 开始同步表 dws_search_keyword_d
2025-08-07 16:48:37,690 - INFO - 表 dws_search_keyword_d 结构同步成功
2025-08-07 16:48:37,984 - INFO - 表 dws_search_keyword_d 数据同步完成，记录数: 1500
2025-08-07 16:48:37,985 - INFO - 表 dws_search_keyword_d 同步成功
2025-08-07 16:48:37,985 - INFO - 开始同步表 dws_sku_sales_d
2025-08-07 16:48:38,179 - INFO - 表 dws_sku_sales_d 结构同步成功
2025-08-07 16:48:38,422 - INFO - 表 dws_sku_sales_d 数据同步完成，记录数: 1439
2025-08-07 16:48:38,423 - INFO - 表 dws_sku_sales_d 同步成功
2025-08-07 16:48:38,423 - INFO - 开始同步表 dws_traffic_source_d
2025-08-07 16:48:38,560 - INFO - 表 dws_traffic_source_d 结构同步成功
2025-08-07 16:48:38,769 - INFO - 表 dws_traffic_source_d 数据同步完成，记录数: 2000
2025-08-07 16:48:38,770 - INFO - 表 dws_traffic_source_d 同步成功
2025-08-07 16:48:38,770 - INFO - 开始同步表 ods_price_power_product
2025-08-07 16:48:38,950 - ERROR - 同步表 ods_price_power_product 结构失败: (1067, "Invalid default value for 'update_time'")
2025-08-07 16:48:38,950 - ERROR - 表 ods_price_power_product 同步失败，跳过继续处理其他表
2025-08-07 16:48:38,950 - INFO - 开始同步表 ods_product_alert
2025-08-07 16:48:39,121 - ERROR - 同步表 ods_product_alert 结构失败: (1067, "Invalid default value for 'update_time'")
2025-08-07 16:48:39,122 - ERROR - 表 ods_product_alert 同步失败，跳过继续处理其他表
2025-08-07 16:48:39,122 - INFO - 开始同步表 ods_product_conversion
2025-08-07 16:48:39,292 - INFO - 表 ods_product_conversion 结构同步成功
2025-08-07 16:48:40,001 - INFO - 表 ods_product_conversion 数据同步完成，记录数: 500
2025-08-07 16:48:40,002 - INFO - 表 ods_product_conversion 同步成功
2025-08-07 16:48:40,002 - INFO - 开始同步表 ods_product_follow
2025-08-07 16:48:40,154 - ERROR - 同步表 ods_product_follow 结构失败: (1067, "Invalid default value for 'etl_time'")
2025-08-07 16:48:40,154 - ERROR - 表 ods_product_follow 同步失败，跳过继续处理其他表
2025-08-07 16:48:40,154 - INFO - 开始同步表 ods_product_info
2025-08-07 16:48:40,324 - ERROR - 同步表 ods_product_info 结构失败: (1067, "Invalid default value for 'update_time'")
2025-08-07 16:48:40,324 - ERROR - 表 ods_product_info 同步失败，跳过继续处理其他表
2025-08-07 16:48:40,324 - INFO - 开始同步表 ods_product_sales
2025-08-07 16:48:40,480 - ERROR - 同步表 ods_product_sales 结构失败: (1067, "Invalid default value for 'etl_time'")
2025-08-07 16:48:40,480 - ERROR - 表 ods_product_sales 同步失败，跳过继续处理其他表
2025-08-07 16:48:40,480 - INFO - 开始同步表 ods_product_sku
2025-08-07 16:48:40,634 - ERROR - 同步表 ods_product_sku 结构失败: (1067, "Invalid default value for 'update_time'")
2025-08-07 16:48:40,634 - ERROR - 表 ods_product_sku 同步失败，跳过继续处理其他表
2025-08-07 16:48:40,634 - INFO - 开始同步表 ods_product_stock
2025-08-07 16:48:40,854 - INFO - 表 ods_product_stock 结构同步成功
2025-08-07 16:48:41,068 - INFO - 表 ods_product_stock 数据同步完成，记录数: 1500
2025-08-07 16:48:41,068 - INFO - 表 ods_product_stock 同步成功
2025-08-07 16:48:41,068 - INFO - 开始同步表 ods_product_traffic
2025-08-07 16:48:41,229 - INFO - 表 ods_product_traffic 结构同步成功
2025-08-07 16:48:41,474 - INFO - 表 ods_product_traffic 数据同步完成，记录数: 2000
2025-08-07 16:48:41,474 - INFO - 表 ods_product_traffic 同步成功
2025-08-07 16:48:41,474 - INFO - 开始同步表 ods_search_keyword
2025-08-07 16:48:41,615 - INFO - 表 ods_search_keyword 结构同步成功
2025-08-07 16:48:41,824 - INFO - 表 ods_search_keyword 数据同步完成，记录数: 1500
2025-08-07 16:48:41,824 - INFO - 表 ods_search_keyword 同步成功
